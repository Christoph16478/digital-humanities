# Projectdocumentation

## 22.12.2018

Setting up the process of getting data from homepage, open it,
count words and exclude special characters from counted words.

Following modules are used in this project:

  from urllib               import request
  from collections          import Counter
  from nltk                 import word_tokenize, FreqDist
  from wordcloud            import WordCloud
  from nltk.corpus          import stopwords

  import numpy              as np
  import matplotlib.pyplot  as plt
  import random
  import nltk
  import requests

## 26.12.2018

Error occured here:
"Failed loading english.pickle with nltk.data.load".

Solution found on stackoverflow for that problem:
https://stackoverflow.com/questions/4867197/failed-loading-english-pickle-with-nltk-data-load
  
  1. Go to a prompt Shell
  2. Write the following Code:
  3. import nltk.data
     nltk.download()
  4. Press 'Enter'
  5. The installation window appears.
  6. Go to the 'Models' tab and select 'Punkt' from the identifier column.
  7. Then click 'Download' and it will install the necessary files.

## 27.12.2018

Reading pandas documentation

Installing finally the following libraries by simply execute following instructions:

  conda install -c anaconda nltk 
  conda install -c ulmo urllib3
  conda install -c conda-forge wordcloud
  conda install -c anaconda numpy
  conda install -c conda-forge matplotlib

## 13.02.2019

Creating a virtual environment and upload the first try in a repository on my github account.

Following sites were very useful:

  https://ipython.readthedocs.io/en/stable(install(kernel_install.html
  https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/

## 17.02.2019

Structuring the whole writting of "README.md" and "documentation.txt"
Structuring means what and how to wirte.

## 20.02.2019

Adding output for specific letters in words.

# 22.02.2018

Adding more specifications and doing some testing.

First final version is done.
